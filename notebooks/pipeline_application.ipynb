{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from spatiospectral_diarization.pipeline import SpatioSpectralDiarizationPipeline\n",
    "import numpy as np\n",
    "import einops\n",
    "from lazy_dataset.database import JsonDatabase\n",
    "import paderbox as pb\n",
    "import torch"
   ],
   "id": "ee99b142fd7c2dc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Application and modification of the diarization pipeline",
   "id": "d253c3be9267a61a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Content of this Notebook\n",
    "This notebook demonstrates how to apply the `SpatioSpectralDiarizationPipeline` to a multi-channel audio signal and how to modify the pipeline for different applications, such as switching from compact to distributed microphone arrays.\n",
    "Additionally, in the later section of the notebook, example code is provided on how to replace and modify individual components of the pipeline."
   ],
   "id": "13d69f383c66401d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initializing the LibriWASN database to get dummy signals from",
   "id": "223cdad1d7275992"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "libriwasn = JsonDatabase('libriwasn.json') # PAth to your version of libriwasn\n",
    "dataset = libriwasn.get_dataset('libriwasn200')  #\n",
    "example_meeting = dataset[51] # A single meeting from the OV40 subset of LibriWASN"
   ],
   "id": "e746bd8da437c334"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading dummy signals (compact and distributed)",
   "id": "dac9f0e306c848fc"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Replace dummy code with example sessions of LibriWASN, will be exchanged in the next update\n",
    "compact_audio_path = example_meeting['audio_path']['osbervation']['asnupb7'] # compact 4-channel device, see documentaiton of LibriWASN for more details\n",
    "audio_compact = pb.io.load_audio(compact_audio_path)\n",
    "distributed_devices = ['Pixel6a', 'Pixel6b', 'Pixel7', 'Xiaomi']  # List of smartphone devices in LibriWASN\n",
    "\n",
    "audio_distributed = [pb.io.load_audio(example_meeting['audio_path']['observation'][device]) for device in distributed_devices]\n",
    "# The Xiaomi device has a flipped phase compared to the Pixel devices, requires phase correction after loading\n",
    "audio_distributed[-1] = -1*audio_distributed[-1]"
   ],
   "id": "d9b98ca1d9b6e93b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Application of the unaltered pipeline",
   "id": "bc2d4c157854f263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By default, the pipeline can be initialized and applied to a multi-channel audio signal to obtain the diarization estimate. Here, the spatial segmentation is parametrized for a compact microphone array, and a ResNet34-based embedding extractor followed by HDBSCAN clustering is used for the diarization.",
   "id": "7dff0aca21a8fd87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ordinary_dia_pipeline = SpatioSpectralDiarizationPipeline()\n",
    "diarization_estimate = ordinary_dia_pipeline(audio_compact)"
   ],
   "id": "78533f1d6501487d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By default, the diarization estimate is in form of a dictionary containing ArrayIntervals for each speaker, which provide an efficient realization of booelan arrays by only storing the start and end indices of the segments.\n",
   "id": "f9bcc3516ea4f0e3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print(diarization_estimate['diarization_estimate'])",
   "id": "9c33760325f83cf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ArrayIntervals can be transformed back into boolean numpy arrays by slicing, i.e.\n",
    "\n",
    "   `dia_array_spk1 = diarization_estimate['diarization_estimate'][0][:600*16_000]`"
   ],
   "id": "f42124fd0ba2ce98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To export the diarization estimate to an .rttm format, you can use the function `pb.array.interval.to_rttm(diarization_estimate['diarization_estimate'], target_path)`",
   "id": "52006dd3cccd35ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pb.array.interval.to_rttm(diarization_estimate['diarization_estimate'], 'dia_estimate_compact.rttm')",
   "id": "d3e6dc10ffd7ef07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with pb.visualization.axes_context(fig_size=(10, 5), columns=2) as ax:\n",
    "    pb.visualization.plot.activity(\n",
    "        diarization_estimate['diarization_estimate'], ax=ax.new, title='Diarization Estimate for Compact Microphone Array')\n",
    "    pb.visualization.plot.activity(\n",
    "        diarization_estimate['diarization_estimate'], ax=ax.new, title='Diarization Target for the dummy signal')"
   ],
   "id": "420bcb0d9395f1e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Application in a distributed microphone setup",
   "id": "755396b25c5c11fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Differences to the compact setup\n",
    "Compared to an application in a compact microphone setup, two major differences need to be taken into account when applying the pipeline to a distributed microphone setup:\n",
    " * The audio signals need to be synchronized before applying the diarization system\n",
    "  * The TDOAs become significantly larger due to the larger inter-microphone distances, which requires a modification of the TDOA estimation and segmentation parameters."
   ],
   "id": "8383f28a147d29a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Synchronization",
   "id": "f730cb57ea3a33c8"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Synchronization of the distributed signals is required for the diarization pipeline to work correctly. Here we use SRO (sampling rate offset) compensation with Dynamic WACD  via the 'paderwasn' package\n",
    "\n",
    "from spatiospectral_diarization.sro_compensation.sync import estimate_sros, compensate_for_sros\n",
    "\n",
    "sros = estimate_sros(audio_distributed)\n",
    "synchronized_audios_distributed = compensate_for_sros(audio_distributed, sros)"
   ],
   "id": "8c1f3c693800e767",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Application with changed segmentation parameters",
   "id": "15b3ed1f8f6a556e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modifying the segmentation parameters for a distributed microphone setup\n",
    "tdoa_settings = {\n",
    "    'max_diff': 2,  # Threshold parameter of delays over a closed microphone loop, larger differences are discarded since physical correc tvalue is 0\n",
    "    'search_range': 200,  # Maximal delay in samples for TDOA estimation that can be tracked\n",
    "    'f_min':125,  # Minimum frequency for TDOA estimation\n",
    "    'f_max': 3500,  # Maximum frequency for TDOA estimation (becomes uninformative for higher frequencies in distributed setup)\n",
    "    'distributed': True,  # Flag to indicate that the TDOA estimation is performed in a distributed setup, only needed if only spatial dia is performed\n",
    "}\n",
    "segmentation_settings= {'max_dist': 2,  # Maximal difference in samples of TDOA vectors to be considered from the same segment\n",
    "                        'peak_ratio_th':.5,\n",
    "                        'max_temp_dist': 16 # Maximal temporal distance between two TDOA vectors s.t. they can belong to the same segment\n",
    "                        }\n",
    "\n",
    "distributed_pipeline = SpatioSpectralDiarizationPipeline(\n",
    "    segmentation_settings=segmentation_settings,\n",
    "    tdoa_settings=tdoa_settings\n",
    ")\n",
    "diarization_estimate = distributed_pipeline(synchronized_audios_distributed)\n"
   ],
   "id": "c607e10b084866a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with pb.visualization.axes_context(fig_size=(10, 5), columns=2) as ax:\n",
    "    pb.visualization.plot.activity(\n",
    "        diarization_estimate['diarization_estimate'], ax=ax.new, title='Diarization Estimate for distributed Microphone Array')\n",
    "    pb.visualization.plot.activity(\n",
    "        diarization_estimate['diarization_estimate'], ax=ax.new, title='Diarization Target for the dummy signal')"
   ],
   "id": "c7c47b8baa000cfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exchanging Modules of the Pipeline",
   "id": "5a8c9f2091d0d30f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "By default, all components of the pipeline can be exchanged without any large overhead. However, since the core aspects lie in the spatial segmentation and segment-level beamforming, the typical use-case should lie in exchanging the VAD, the embedding extractor, or the clustering stage.\n",
    "\n",
    "These modules are structured in a way, that the pipeline class expects a callable object (e.g. a class or function) for each of these components.\n",
    "\n",
    "In the following, we provide some mock classes on how a modified VAD, embedding extractor or clusering might look like."
   ],
   "id": "1b0c6c545d89a6f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example VAD module",
   "id": "ecae44953bc0625e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def my_new_vad(recording):\n",
    "    \"\"\"\n",
    "    A mock VAD module that returns a channel-wise VAD with frame-resolution matching the STFT for the GCC computation.\n",
    "    Args:\n",
    "        recording (np.ndarray): The input audio recording with shape (num_channels, num_samples).\n",
    "\n",
    "    Returns:\n",
    "        vad_estimate (np.ndarray): A boolean array of shape (num_channels, num_samples) indicating the VAD decision for each samples.\n",
    "    \"\"\"\n",
    "\n",
    "    # Very simplistic energy-based VAD tracking the average power of the recording and thresholding on this value\n",
    "    vad_estimate = np.zeros_like(recording)\n",
    "    for ch_idx, signal in enumerate(recording):\n",
    "        avg_power = np.mean(signal ** 2)\n",
    "        threshold = avg_power * 0.5  # Set threshold to 50% of the avg power\n",
    "        sample_power = signal ** 2\n",
    "        vad_estimate[ch_idx,:] = sample_power > threshold\n",
    "    return vad_estimate"
   ],
   "id": "c753cdfda4f6dee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example Embedding Extractor",
   "id": "7866630ffeff8630"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from speechbrain.inference.speaker import EncoderClassifier\n",
    "\n",
    "class MyNewEmbeddingExtractor:\n",
    "    \"\"\"\n",
    "    A mock embedding extractor that outputs a speaker embedding from a single audio segment in time domain. In this case, this class simply wraps the call instructions of the ECAPA-TDNN from speechbrain, see https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Simply initializes the pre-trained ECAPA-TDNN model from speechbrain.\n",
    "        \"\"\"\n",
    "        self.embedding_extractor = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "    def __call__(self, audio_segment):\n",
    "        \"\"\"\n",
    "        Extracts the speaker embedding for the input recording.\n",
    "        Args:\n",
    "            audio_segment (np.ndarray): 1-dimensional audio segment with shape (num_samples,).\n",
    "\n",
    "        Returns:\n",
    "            embedding (np.ndarray): E-dimenional speaker embedding for the given audio segment, shape(E,).\n",
    "        \"\"\"\n",
    "        # Convert the numpy array to a torch tensor\n",
    "        recording_tensor = torch.tensor(audio_segment, dtype=torch.float32)\n",
    "        # Extract the embeddings using the speechbrain model\n",
    "        with torch.no_grad():\n",
    "            embedding = self.embedding_extractor.encode_batch(recording_tensor)\n",
    "            embedding = torch.mean(embedding, dim=0)  # Average over the batch dimension\n",
    "        embedding = embedding.detach().numpy()  # Convert back to numpy array\n",
    "        return embedding"
   ],
   "id": "3afbf5419735f58e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example Clustering Module",
   "id": "55e3a2b22ead5205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def my_new_clustering(embeddings, seg_boundaries):\n",
    "    \"\"\"\n",
    "    A mock clustering module that performs clustering on the given embeddings.\n",
    "    Args:\n",
    "        embeddings (np.ndarray): An array of shape (num_segments, embedding_dim) containing the speaker embeddings of all segments\n",
    "        seg_boundaries (list): The segment_boundaries (start, end) of all segments. Can be used jointly with the embeddings, e.g. for importance weighting of embeddings, or to discard too short segments for clustering\n",
    "\n",
    "    Returns:\n",
    "        labels (np.ndarray): An array of shape (num_segments,) containing the cluster labels for each segment.\n",
    "        segment_boundaries_new (list): The segment boundaries of the segments after clusering. Allows discarding and modification of segments during clustering\n",
    "        embeddings_new (np.ndarray): The embeddings of the segments after clustering. Allows discarding and modification of segments during clustering\n",
    "    \"\"\"\n",
    "    # For simplicity, we use a dummy clustering method that assigns all segments to a  random cluster\n",
    "    num_clusters = 8\n",
    "    labels = np.random.randint(0, num_clusters, size=embeddings.shape[0])\n",
    "    # Since no segments are discarded or manipulated, the input segments ar simply the output\n",
    "    return labels, seg_boundaries, embeddings"
   ],
   "id": "4cf0d7573eec148f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Constructing and applying the new custom pipeline",
   "id": "5c1587671dc2c205"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "custom_pipeline = SpatioSpectralDiarizationPipeline(\n",
    "    vad_module=my_new_vad,\n",
    "    embedding_extractor=MyNewEmbeddingExtractor(),\n",
    "    clustering=my_new_clustering,  # or any other clustering method\n",
    ")\n",
    "custom_diarization_estimate = custom_pipeline(audio_compact)"
   ],
   "id": "51d0cc8ee6250c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2ed22872d435ae61"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
